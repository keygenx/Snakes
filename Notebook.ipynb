{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from snakeai.gameplay.environment import Environment\n",
    "from snakeai.gui import PyGameGUI\n",
    "from snakeai.utils.cli import HelpOnFailArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snake_environment(level_filename):\n",
    "    \"\"\" Create a new Snake environment from the config file. \"\"\"\n",
    "\n",
    "    with open(level_filename) as cfg:\n",
    "        env_config = json.load(cfg)\n",
    "    print(env_config)\n",
    "    return Environment(config=env_config, verbose=1)\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\" Load a pre-trained agent model. \"\"\"\n",
    "\n",
    "    \n",
    "    return tf.keras.models.load_model(filename)\n",
    "\n",
    "\n",
    "def create_agent(name, model):\n",
    "    \"\"\"\n",
    "    Create a specific type of Snake AI agent.\n",
    "    \n",
    "    Args:\n",
    "        name (str): key identifying the agent type.\n",
    "        model: (optional) a pre-trained model required by certain agents.\n",
    "\n",
    "    Returns:\n",
    "        An instance of Snake agent.\n",
    "    \"\"\"\n",
    "\n",
    "    from snakeai.agent import DeepQNetworkAgent, HumanAgent, RandomActionAgent\n",
    "\n",
    "    if name == 'human':\n",
    "        return HumanAgent()\n",
    "    elif name == 'dqn':\n",
    "        if model is None:\n",
    "            raise ValueError('A model file is required for a DQN agent.')\n",
    "        return DeepQNetworkAgent(model=model, memory_size=-1, num_last_frames=4)\n",
    "    elif name == 'random':\n",
    "        return RandomActionAgent()\n",
    "\n",
    "    raise KeyError(f'Unknown agent type: \"{name}\"')\n",
    "\n",
    "\n",
    "def play_cli(env, agent, num_episodes=10):\n",
    "    \"\"\"\n",
    "    Play a set of episodes using the specified Snake agent.\n",
    "    Use the non-interactive command-line interface and print the summary statistics afterwards.\n",
    "    \n",
    "    Args:\n",
    "        env: an instance of Snake environment.\n",
    "        agent: an instance of Snake agent.\n",
    "        num_episodes (int): the number of episodes to run.\n",
    "    \"\"\"\n",
    "\n",
    "    fruit_stats = []\n",
    "\n",
    "    print()\n",
    "    print('Playing:')\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        timestep = env.new_episode()\n",
    "        agent.begin_episode()\n",
    "        game_over = False\n",
    "\n",
    "        while not game_over:\n",
    "            action = agent.act(timestep.observation, timestep.reward)\n",
    "            env.choose_action(action)\n",
    "            timestep = env.timestep()\n",
    "            game_over = timestep.is_episode_end\n",
    "\n",
    "        fruit_stats.append(env.stats.fruits_eaten)\n",
    "\n",
    "        summary = 'Episode {:3d} / {:3d} | Timesteps {:4d} | Fruits {:2d}'\n",
    "        print(summary.format(episode + 1, num_episodes, env.stats.timesteps_survived, env.stats.fruits_eaten))\n",
    "\n",
    "    print()\n",
    "    print('Fruits eaten {:.1f} +/- stddev {:.1f}'.format(np.mean(fruit_stats), np.std(fruit_stats)))\n",
    "\n",
    "\n",
    "def play_gui(env, agent, num_episodes):\n",
    "    \"\"\"\n",
    "    Play a set of episodes using the specified Snake agent.\n",
    "    Use the interactive graphical interface.\n",
    "    \n",
    "    Args:\n",
    "        env: an instance of Snake environment.\n",
    "        agent: an instance of Snake agent.\n",
    "        num_episodes (int): the number of episodes to run.\n",
    "    \"\"\"\n",
    "\n",
    "    gui = PyGameGUI()\n",
    "    gui.load_environment(env)\n",
    "    gui.load_agent(agent)\n",
    "    gui.run(num_episodes=num_episodes)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'field': ['#############', '#...........#', '#...........#', '#...........#', '#....S......#', '#...........#', '#...........#', '#...........#', '#...........#', '#############'], 'initial_snake_length': 3, 'max_step_limit': 1000, 'rewards': {'timestep': 0, 'ate_fruit': 1, 'died': -1}}\n"
     ]
    }
   ],
   "source": [
    "    env = create_snake_environment(r\"snakeai/levels/10x10-blank.json\")\n",
    "    #model = load_model(\"dqn-10x10-blank.model\") \n",
    "    model = None\n",
    "    agent = create_agent('human', model)\n",
    "\n",
    "    run_player = play_gui\n",
    "    run_player(env, agent, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class person:\n",
    "    def __init__(self):\n",
    "        self.__name=''\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__name\n",
    "    @name.setter\n",
    "    def name(self, value):\n",
    "        self.__name=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ass'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =person()\n",
    "a.name = 'ass'\n",
    "a.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "from snakeai.agent import DeepQNetworkAgent\n",
    "from snakeai.gameplay.environment import Environment\n",
    "from snakeai.utils.cli import HelpOnFailArgumentParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snake_environment(level_filename):\n",
    "    \"\"\" Create a new Snake environment from the config file. \"\"\"\n",
    "\n",
    "    with open(level_filename) as cfg:\n",
    "        env_config = json.load(cfg)\n",
    "\n",
    "    return Environment(config=env_config, verbose=1)\n",
    "\n",
    "\n",
    "def create_dqn_model(env, num_last_frames):\n",
    "    \"\"\"\n",
    "    Build a new DQN model to be used for training.\n",
    "    \n",
    "    Args:\n",
    "        env: an instance of Snake environment. \n",
    "        num_last_frames: the number of last frames the agent considers as state.\n",
    "\n",
    "    Returns:\n",
    "        A compiled DQN model.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutions.\n",
    "    model.add(Conv2D(\n",
    "        16,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        data_format='channels_first',\n",
    "        input_shape=(num_last_frames, ) + env.observation_shape\n",
    "    ))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(\n",
    "        32,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        data_format='channels_first'\n",
    "    ))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Dense layers.\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(env.num_actions))\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(RMSprop(), 'MSE')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 16, 8, 8)          592       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 6, 6)          4640      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 32, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               295168    \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 301,171\n",
      "Trainable params: 301,171\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c06a3deb00ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mnum_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mcheckpoint_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_episodes\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdiscount_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.95\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m )\n",
      "\u001b[1;32m~\\OneDrive - University of Bath\\Reinforcement Learning\\CW 2\\Snake Environment\\snakeai\\agent\\dqn.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, env, num_episodes, batch_size, discount_factor, checkpoint_freq, exploration_range, exploration_phase_size)\u001b[0m\n\u001b[0;32m    111\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                     \u001b[0mdiscount_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdiscount_factor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m                 )\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Learn on the batch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - University of Bath\\Reinforcement Learning\\CW 2\\Snake Environment\\snakeai\\utils\\memory.py\u001b[0m in \u001b[0;36mget_batch\u001b[1;34m(self, model, batch_size, discount_factor)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mdelta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdelta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdiscount_factor\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mepisode_ends\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mQ_next\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 4 is out of bounds for axis 1 with size 3"
     ]
    }
   ],
   "source": [
    "num_episodes = 10000\n",
    "env = create_snake_environment(r\"snakeai/levels/10x10-blank.json\")\n",
    "model = create_dqn_model(env, num_last_frames=4)\n",
    "\n",
    "agent = DeepQNetworkAgent(\n",
    "    model=model,\n",
    "    memory_size=-1,\n",
    "    num_last_frames=model.input_shape[1]\n",
    ")\n",
    "\n",
    "agent.train(\n",
    "    env,\n",
    "    batch_size=64,\n",
    "    num_episodes=num_episodes,\n",
    "    checkpoint_freq=num_episodes // 10,\n",
    "    discount_factor=0.95\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
