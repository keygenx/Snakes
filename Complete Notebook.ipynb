{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class copied from snakeai\\gameplay\\environment.py\n",
    "\n",
    "from snakeai.gameplay.entities import Snake, Field, CellType, SnakeAction, ALL_SNAKE_ACTIONS\n",
    "import pprint\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Environment(object):\n",
    "    \"\"\"\n",
    "    Represents the RL environment for the Snake game that implements the game logic,\n",
    "    provides rewards for the agent and keeps track of game statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, verbose=1):\n",
    "        \"\"\"\n",
    "        Create a new Snake RL environment.\n",
    "        \n",
    "        Args:\n",
    "            config (dict): level configuration, typically found in JSON configs.  \n",
    "            verbose (int): verbosity level:\n",
    "                0 = do not write any debug information;\n",
    "                1 = write a CSV file containing the statistics for every episode;\n",
    "                2 = same as 1, but also write a full log file containing the state of each timestep.\n",
    "        \"\"\"\n",
    "        self.field = Field(level_map=config['field'])\n",
    "        self.snake = None\n",
    "        self.fruit = None\n",
    "        self.initial_snake_length = config['initial_snake_length']\n",
    "        self.rewards = config['rewards']\n",
    "        self.max_step_limit = config.get('max_step_limit', 1000)\n",
    "        self.is_game_over = False\n",
    "\n",
    "        self.timestep_index = 0\n",
    "        self.current_action = None\n",
    "        self.stats = EpisodeStatistics()\n",
    "        self.verbose = verbose\n",
    "        self.debug_file = None\n",
    "        self.stats_file = None\n",
    "\n",
    "    def seed(self, value):\n",
    "        \"\"\" Initialize the random state of the environment to make results reproducible. \"\"\"\n",
    "        random.seed(value)\n",
    "        np.random.seed(value)\n",
    "\n",
    "    @property\n",
    "    def observation_shape(self):\n",
    "        \"\"\" Get the shape of the state observed at each timestep. \"\"\"\n",
    "        return self.field.size_x, self.field.size_y\n",
    "\n",
    "    @property\n",
    "    def num_actions(self):\n",
    "        \"\"\" Get the number of actions the agent can take. \"\"\"\n",
    "        return len(ALL_SNAKE_ACTIONS)\n",
    "\n",
    "    def new_episode(self):\n",
    "        \"\"\" Reset the environment and begin a new episode. \"\"\"\n",
    "        self.field.create_level()\n",
    "        self.stats.reset()\n",
    "        self.timestep_index = 0\n",
    "\n",
    "        self.snake = Snake(self.field.find_snake_head(), length=self.initial_snake_length)\n",
    "        self.field.place_snake(self.snake)\n",
    "        self.generate_fruit()\n",
    "        self.current_action = None\n",
    "        self.is_game_over = False\n",
    "\n",
    "        result = TimestepResult(\n",
    "            observation=self.get_observation(),\n",
    "            reward=0,\n",
    "            is_episode_end=self.is_game_over\n",
    "        )\n",
    "\n",
    "        self.record_timestep_stats(result)\n",
    "        return result\n",
    "\n",
    "    def record_timestep_stats(self, result):\n",
    "        \"\"\" Record environment statistics according to the verbosity level. \"\"\"\n",
    "        timestamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "        # Write CSV header for the stats file.\n",
    "        if self.verbose >= 1 and self.stats_file is None:\n",
    "            self.stats_file = open(f'debug_stats/snake-env-{timestamp}.csv', 'w')\n",
    "            stats_csv_header_line = self.stats.to_dataframe()[:0].to_csv(index=None)\n",
    "            print(stats_csv_header_line, file=self.stats_file, end='', flush=True)\n",
    "\n",
    "        # Create a blank debug log file.\n",
    "        if self.verbose >= 2 and self.debug_file is None:\n",
    "            self.debug_file = open(f'debug_log/snake-env-{timestamp}.log', 'w')\n",
    "\n",
    "        self.stats.record_timestep(self.current_action, result)\n",
    "        self.stats.timesteps_survived = self.timestep_index\n",
    "\n",
    "        if self.verbose >= 2:\n",
    "            print(result, file=self.debug_file)\n",
    "\n",
    "        # Log episode stats if the appropriate verbosity level is set.\n",
    "        if result.is_episode_end:\n",
    "            if self.verbose >= 1:\n",
    "                stats_csv_line = self.stats.to_dataframe().to_csv(header=False, index=None)\n",
    "                print(stats_csv_line, file=self.stats_file, end='', flush=True)\n",
    "            if self.verbose >= 2:\n",
    "                print(self.stats, file=self.debug_file)\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\" Observe the state of the environment. \"\"\"\n",
    "        return np.copy(self.field._cells)\n",
    "\n",
    "    def choose_action(self, action):\n",
    "        \"\"\" Choose the action that will be taken at the next timestep. \"\"\"\n",
    "\n",
    "        self.current_action = action\n",
    "        if action == SnakeAction.TURN_LEFT:\n",
    "            self.snake.turn_left()\n",
    "        elif action == SnakeAction.TURN_RIGHT:\n",
    "            self.snake.turn_right()\n",
    "\n",
    "    def timestep(self):\n",
    "        \"\"\" Execute the timestep and return the new observable state. \"\"\"\n",
    "\n",
    "        self.timestep_index += 1\n",
    "        reward = 0\n",
    "\n",
    "        old_head = self.snake.head\n",
    "        old_tail = self.snake.tail\n",
    "\n",
    "        # Are we about to eat the fruit?\n",
    "        if self.snake.peek_next_move() == self.fruit:\n",
    "            self.snake.grow()\n",
    "            self.generate_fruit()\n",
    "            old_tail = None\n",
    "            reward += self.rewards['ate_fruit'] * self.snake.length\n",
    "            self.stats.fruits_eaten += 1\n",
    "\n",
    "        # If not, just move forward.\n",
    "        else:\n",
    "            self.snake.move()\n",
    "            reward += self.rewards['timestep']\n",
    "\n",
    "        self.field.update_snake_footprint(old_head, old_tail, self.snake.head)\n",
    "\n",
    "        # Hit a wall or own body?\n",
    "        if not self.is_alive():\n",
    "            if self.has_hit_wall():\n",
    "                self.stats.termination_reason = 'hit_wall'\n",
    "            if self.has_hit_own_body():\n",
    "                self.stats.termination_reason = 'hit_own_body'\n",
    "\n",
    "            self.field[self.snake.head] = CellType.SNAKE_HEAD\n",
    "            self.is_game_over = True\n",
    "            reward = self.rewards['died']\n",
    "\n",
    "        # Exceeded the limit of moves?\n",
    "        if self.timestep_index >= self.max_step_limit:\n",
    "            self.is_game_over = True\n",
    "            self.stats.termination_reason = 'timestep_limit_exceeded'\n",
    "\n",
    "        result = TimestepResult(\n",
    "            observation=self.get_observation(),\n",
    "            reward=reward,\n",
    "            is_episode_end=self.is_game_over\n",
    "        )\n",
    "\n",
    "        self.record_timestep_stats(result)\n",
    "        return result\n",
    "\n",
    "    def generate_fruit(self, position=None):\n",
    "        \"\"\" Generate a new fruit at a random unoccupied cell. \"\"\"\n",
    "        if position is None:\n",
    "            position = self.field.get_random_empty_cell()\n",
    "        self.field[position] = CellType.FRUIT\n",
    "        self.fruit = position\n",
    "\n",
    "    def has_hit_wall(self):\n",
    "        \"\"\" True if the snake has hit a wall, False otherwise. \"\"\"\n",
    "        return self.field[self.snake.head] == CellType.WALL\n",
    "\n",
    "    def has_hit_own_body(self):\n",
    "        \"\"\" True if the snake has hit its own body, False otherwise. \"\"\"\n",
    "        return self.field[self.snake.head] == CellType.SNAKE_BODY\n",
    "\n",
    "    def is_alive(self):\n",
    "        \"\"\" True if the snake is still alive, False otherwise. \"\"\"\n",
    "        return not self.has_hit_wall() and not self.has_hit_own_body()\n",
    "\n",
    "\n",
    "class TimestepResult(object):\n",
    "    \"\"\" Represents the information provided to the agent after each timestep. \"\"\"\n",
    "\n",
    "    def __init__(self, observation, reward, is_episode_end):\n",
    "        self.observation = observation\n",
    "        self.reward = reward\n",
    "        self.is_episode_end = is_episode_end\n",
    "\n",
    "    def __str__(self):\n",
    "        field_map = '\\n'.join([\n",
    "            ''.join(str(cell) for cell in row)\n",
    "            for row in self.observation\n",
    "        ])\n",
    "        return f'{field_map}\\nR = {self.reward}   end={self.is_episode_end}\\n'\n",
    "\n",
    "\n",
    "class EpisodeStatistics(object):\n",
    "    \"\"\" Represents the summary of the agent's performance during the episode. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Forget all previous statistics and prepare for a new episode. \"\"\"\n",
    "        self.timesteps_survived = 0\n",
    "        self.sum_episode_rewards = 0\n",
    "        self.fruits_eaten = 0\n",
    "        self.termination_reason = None\n",
    "        self.action_counter = {\n",
    "            action: 0\n",
    "            for action in ALL_SNAKE_ACTIONS\n",
    "        }\n",
    "\n",
    "    def record_timestep(self, action, result):\n",
    "        \"\"\" Update the stats based on the current timestep results. \"\"\"\n",
    "        self.sum_episode_rewards += result.reward\n",
    "        if action is not None:\n",
    "            self.action_counter[action] += 1\n",
    "\n",
    "    def flatten(self):\n",
    "        \"\"\" Format all episode statistics as a flat object. \"\"\"\n",
    "        flat_stats = {\n",
    "            'timesteps_survived': self.timesteps_survived,\n",
    "            'sum_episode_rewards': self.sum_episode_rewards,\n",
    "            'mean_reward': self.sum_episode_rewards / self.timesteps_survived if self.timesteps_survived else None,\n",
    "            'fruits_eaten': self.fruits_eaten,\n",
    "            'termination_reason': self.termination_reason,\n",
    "        }\n",
    "        flat_stats.update({\n",
    "            f'action_counter_{action}': self.action_counter.get(action, 0)\n",
    "            for action in ALL_SNAKE_ACTIONS\n",
    "        })\n",
    "        return flat_stats\n",
    "\n",
    "    def to_dataframe(self):\n",
    "        \"\"\" Convert the episode statistics to a Pandas data frame. \"\"\"\n",
    "        return pd.DataFrame([self.flatten()])\n",
    "\n",
    "    def __str__(self):\n",
    "        return pprint.pformat(self.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions and Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snake_environment(level_filename):\n",
    "    \"\"\" Create a new Snake environment from the config file. \"\"\"\n",
    "\n",
    "    with open(level_filename) as cfg:\n",
    "        env_config = json.load(cfg)\n",
    "\n",
    "    return Environment(config=env_config, verbose=1)\n",
    "\n",
    "\n",
    "def create_dqn_model(env, num_last_frames):\n",
    "    \"\"\"\n",
    "    Build a new DQN model to be used for training.\n",
    "    \n",
    "    Args:\n",
    "        env: an instance of Snake environment. \n",
    "        num_last_frames: the number of last frames the agent considers as state.\n",
    "\n",
    "    Returns:\n",
    "        A compiled DQN model.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutions.\n",
    "    model.add(Conv2D(\n",
    "        16,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        data_format='channels_first',\n",
    "        input_shape=(num_last_frames, ) + env.observation_shape\n",
    "    ))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(\n",
    "        32,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        data_format='channels_first'\n",
    "    ))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Dense layers.\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(env.num_actions))\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(RMSprop(), 'MSE')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExperienceReplay Class from snakeai\\utils\\memory.py\n",
    "import collections #To use Deque datatype for buffer.\n",
    "import random #for random sampling from the buffer. Buffer is a Deque datatype\n",
    "\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    \"\"\" Represents the experience replay memory that can be randomly sampled. \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape, num_actions, memory_size=100):\n",
    "        \"\"\"\n",
    "        Create a new instance of experience replay memory.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: the shape of the agent state.\n",
    "            num_actions: the number of actions allowed in the environment.\n",
    "            memory_size: memory size limit (-1 for unlimited).\n",
    "        \"\"\"\n",
    "        self.memory = collections.deque()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.memory_size = memory_size\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Erase the experience replay memory. \"\"\"\n",
    "        self.memory = collections.deque()\n",
    "\n",
    "    def remember(self, state, action, reward, state_next, is_episode_end):\n",
    "        \"\"\"\n",
    "        Store a new piece of experience into the replay memory.\n",
    "        \n",
    "        Args:\n",
    "            state: state observed at the previous step.\n",
    "            action: action taken at the previous step.\n",
    "            reward: reward received at the beginning of the current step.\n",
    "            state_next: state observed at the current step. \n",
    "            is_episode_end: whether the episode has ended with the current step.\n",
    "        \"\"\"\n",
    "        memory_item = np.concatenate([\n",
    "            state.flatten(),\n",
    "            np.array(action).flatten(),\n",
    "            np.array(reward).flatten(),\n",
    "            state_next.flatten(),\n",
    "            1 * np.array(is_episode_end).flatten()\n",
    "        ])\n",
    "        self.memory.append(memory_item)\n",
    "        if 0 < self.memory_size < len(self.memory):\n",
    "            self.memory.popleft()\n",
    "\n",
    "    def get_batch(self, model, batch_size, discount_factor=0.9):\n",
    "        \"\"\" Sample a batch from experience replay. \"\"\"\n",
    "\n",
    "        batch_size = min(len(self.memory), batch_size)\n",
    "        experience = np.array(random.sample(self.memory, batch_size))\n",
    "        input_dim = np.prod(self.input_shape)\n",
    "\n",
    "        # Extract [S, a, r, S', end] from experience.\n",
    "        states = experience[:, 0:input_dim]\n",
    "        actions = experience[:, input_dim]\n",
    "        rewards = experience[:, input_dim + 1]\n",
    "        states_next = experience[:, input_dim + 2:2 * input_dim + 2]\n",
    "        episode_ends = experience[:, 2 * input_dim + 2]\n",
    "\n",
    "        # Reshape to match the batch structure.\n",
    "        states = states.reshape((batch_size, ) + self.input_shape)\n",
    "        actions = np.cast['int'](actions)\n",
    "        rewards = rewards.repeat(self.num_actions).reshape((batch_size, self.num_actions))\n",
    "        states_next = states_next.reshape((batch_size, ) + self.input_shape)\n",
    "        episode_ends = episode_ends.repeat(self.num_actions).reshape((batch_size, self.num_actions))\n",
    "\n",
    "        # Predict future state-action values.\n",
    "        X = np.concatenate([states, states_next], axis=0)\n",
    "        y = model.predict(X)\n",
    "        Q_next = np.max(y[batch_size:], axis=1).repeat(self.num_actions).reshape((batch_size, self.num_actions))\n",
    "\n",
    "        delta = np.zeros((batch_size, self.num_actions))\n",
    "        delta[np.arange(batch_size), actions] = 1\n",
    "\n",
    "        targets = (1 - delta) * y[:batch_size] + delta * (rewards + discount_factor * (1 - episode_ends) * Q_next)\n",
    "        return states, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DeepQNetworkAgent Class from snakeai\\agent\\dqn.py\n",
    "class DeepQNetworkAgent():\n",
    "    \"\"\" Represents a Snake agent powered by DQN with experience replay. \"\"\"\n",
    "\n",
    "    def __init__(self, model, num_last_frames=4, memory_size=1000):\n",
    "        \"\"\"\n",
    "        Create a new DQN-based agent.\n",
    "        \n",
    "        Args:\n",
    "            model: a compiled DQN model.\n",
    "            num_last_frames (int): the number of last frames the agent will consider.\n",
    "            memory_size (int): memory size limit for experience replay (-1 for unlimited). \n",
    "        \"\"\"\n",
    "        assert model.input_shape[1] == num_last_frames, 'Model input shape should be (num_frames, grid_size, grid_size)'\n",
    "        assert len(model.output_shape) == 2, 'Model output shape should be (num_samples, num_actions)'\n",
    "\n",
    "        self.model = model\n",
    "        self.num_last_frames = num_last_frames\n",
    "        self.memory = ExperienceReplay((num_last_frames,) + model.input_shape[-2:], model.output_shape[-1], memory_size)\n",
    "        self.frames = None\n",
    "\n",
    "    def begin_episode(self):\n",
    "        \"\"\" Reset the agent for a new episode. \"\"\"\n",
    "        self.frames = None\n",
    "\n",
    "    def get_last_frames(self, observation):\n",
    "        \"\"\"\n",
    "        Get the pixels of the last `num_last_frames` observations, the current frame being the last.\n",
    "        \n",
    "        Args:\n",
    "            observation: observation at the current timestep. \n",
    "\n",
    "        Returns:\n",
    "            Observations for the last `num_last_frames` frames.\n",
    "        \"\"\"\n",
    "        frame = observation\n",
    "        if self.frames is None:\n",
    "            self.frames = collections.deque([frame] * self.num_last_frames)\n",
    "        else:\n",
    "            self.frames.append(frame)\n",
    "            self.frames.popleft()\n",
    "        return np.expand_dims(self.frames, 0)\n",
    "\n",
    "    def train(self, env, num_episodes=1000, batch_size=50, discount_factor=0.9, checkpoint_freq=None,\n",
    "              exploration_range=(1.0, 0.1), exploration_phase_size=0.5):\n",
    "        \"\"\"\n",
    "        Train the agent to perform well in the given Snake environment.\n",
    "        \n",
    "        Args:\n",
    "            env:\n",
    "                an instance of Snake environment.\n",
    "            num_episodes (int):\n",
    "                the number of episodes to run during the training.\n",
    "            batch_size (int):\n",
    "                the size of the learning sample for experience replay.\n",
    "            discount_factor (float):\n",
    "                discount factor (gamma) for computing the value function.\n",
    "            checkpoint_freq (int):\n",
    "                the number of episodes after which a new model checkpoint will be created.\n",
    "            exploration_range (tuple):\n",
    "                a (max, min) range specifying how the exploration rate should decay over time. \n",
    "            exploration_phase_size (float):\n",
    "                the percentage of the training process at which\n",
    "                the exploration rate should reach its minimum.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the constant exploration decay speed for each episode.\n",
    "        max_exploration_rate, min_exploration_rate = exploration_range\n",
    "        exploration_decay = ((max_exploration_rate - min_exploration_rate) / (num_episodes * exploration_phase_size))\n",
    "        exploration_rate = max_exploration_rate\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            # Reset the environment for the new episode.\n",
    "            timestep = env.new_episode()\n",
    "            self.begin_episode()\n",
    "            game_over = False\n",
    "            loss = 0.0\n",
    "\n",
    "            # Observe the initial state.\n",
    "            state = self.get_last_frames(timestep.observation)\n",
    "\n",
    "            while not game_over:\n",
    "                if np.random.random() < exploration_rate:\n",
    "                    # Explore: take a random action.\n",
    "                    action = np.random.randint(env.num_actions)\n",
    "                else:\n",
    "                    # Exploit: take the best known action for this state.\n",
    "                    q = self.model.predict(state)\n",
    "                    action = np.argmax(q[0])\n",
    "\n",
    "                # Act on the environment.\n",
    "                env.choose_action(action)\n",
    "                timestep = env.timestep()\n",
    "\n",
    "                # Remember a new piece of experience.\n",
    "                reward = timestep.reward\n",
    "                state_next = self.get_last_frames(timestep.observation)\n",
    "                game_over = timestep.is_episode_end\n",
    "                experience_item = [state, action, reward, state_next, game_over]\n",
    "                self.memory.remember(*experience_item)\n",
    "                state = state_next\n",
    "\n",
    "                # Sample a random batch from experience.\n",
    "                batch = self.memory.get_batch(\n",
    "                    model=self.model,\n",
    "                    batch_size=batch_size,\n",
    "                    discount_factor=discount_factor\n",
    "                )\n",
    "\n",
    "                # Learn on the batch.\n",
    "                if batch:\n",
    "                    inputs, targets = batch\n",
    "                    loss += float(self.model.train_on_batch(inputs, targets))\n",
    "\n",
    "            if checkpoint_freq and (episode % checkpoint_freq) == 0:\n",
    "                self.model.save(f'saved_model/dqn-{episode:08d}.h5')\n",
    "\n",
    "            if exploration_rate > min_exploration_rate:\n",
    "                exploration_rate -= exploration_decay\n",
    "\n",
    "            summary = 'Episode {:5d}/{:5d} | Loss {:8.4f} | Exploration {:.2f} | ' + \\\n",
    "                      'Fruits {:2d} | Timesteps {:4d} | Total Reward {:4d}'\n",
    "            print(summary.format(\n",
    "                episode + 1, num_episodes, loss, exploration_rate,\n",
    "                env.stats.fruits_eaten, env.stats.timesteps_survived, env.stats.sum_episode_rewards,\n",
    "            ))\n",
    "\n",
    "        self.model.save('saved_model/dqn-final.h5')\n",
    "\n",
    "    def act(self, observation, reward):\n",
    "        \"\"\"\n",
    "        Choose the next action to take.\n",
    "        \n",
    "        Args:\n",
    "            observation: observable state for the current timestep. \n",
    "            reward: reward received at the beginning of the current timestep.\n",
    "\n",
    "        Returns:\n",
    "            The index of the action to take next.\n",
    "        \"\"\"\n",
    "        state = self.get_last_frames(observation)\n",
    "        q = self.model.predict(state)[0]\n",
    "        return np.argmax(q)\n",
    "    \n",
    "    def end_episode(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 16, 8, 8)          160       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 32, 6, 6)          4640      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 32, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               295168    \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 300,739\n",
      "Trainable params: 300,739\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode     1/  100 | Loss 250.6310 | Exploration 0.98 | Fruits  0 | Timesteps   13 | Total Reward -112\n",
      "Episode     2/  100 | Loss 1695.8296 | Exploration 0.96 | Fruits  0 | Timesteps   16 | Total Reward -115\n",
      "Episode     3/  100 | Loss 285.8209 | Exploration 0.95 | Fruits  0 | Timesteps    5 | Total Reward -104\n",
      "Episode     4/  100 | Loss 358.4094 | Exploration 0.93 | Fruits  0 | Timesteps    6 | Total Reward -105\n",
      "Episode     5/  100 | Loss 706.4496 | Exploration 0.91 | Fruits  0 | Timesteps    8 | Total Reward -107\n",
      "Episode     6/  100 | Loss 482.1189 | Exploration 0.89 | Fruits  0 | Timesteps    7 | Total Reward -106\n",
      "Episode     7/  100 | Loss 219.2808 | Exploration 0.87 | Fruits  0 | Timesteps    7 | Total Reward -106\n",
      "Episode     8/  100 | Loss 341.3959 | Exploration 0.86 | Fruits  0 | Timesteps   17 | Total Reward -116\n",
      "Episode     9/  100 | Loss 482.9021 | Exploration 0.84 | Fruits  0 | Timesteps   25 | Total Reward -124\n",
      "Episode    10/  100 | Loss 209.5167 | Exploration 0.82 | Fruits  0 | Timesteps    6 | Total Reward -105\n",
      "Episode    11/  100 | Loss 382.8925 | Exploration 0.80 | Fruits  0 | Timesteps   11 | Total Reward -110\n",
      "Episode    12/  100 | Loss 231.6937 | Exploration 0.78 | Fruits  0 | Timesteps    6 | Total Reward -105\n",
      "Episode    13/  100 | Loss 269.8261 | Exploration 0.77 | Fruits  0 | Timesteps    7 | Total Reward -106\n",
      "Episode    14/  100 | Loss 1494.4194 | Exploration 0.75 | Fruits  0 | Timesteps   28 | Total Reward -127\n",
      "Episode    15/  100 | Loss 15200.1732 | Exploration 0.73 | Fruits  2 | Timesteps   50 | Total Reward  753\n",
      "Episode    16/  100 | Loss 19475.6401 | Exploration 0.71 | Fruits  0 | Timesteps   10 | Total Reward -109\n",
      "Episode    17/  100 | Loss 38644.7701 | Exploration 0.69 | Fruits  1 | Timesteps   19 | Total Reward  283\n",
      "Episode    18/  100 | Loss 18416.2169 | Exploration 0.68 | Fruits  0 | Timesteps    9 | Total Reward -108\n",
      "Episode    19/  100 | Loss 82632.3934 | Exploration 0.66 | Fruits  1 | Timesteps   30 | Total Reward  272\n",
      "Episode    20/  100 | Loss 24427.5747 | Exploration 0.64 | Fruits  0 | Timesteps    8 | Total Reward -107\n",
      "Episode    21/  100 | Loss 28047.9468 | Exploration 0.62 | Fruits  0 | Timesteps    9 | Total Reward -108\n",
      "Episode    22/  100 | Loss 51320.9221 | Exploration 0.60 | Fruits  0 | Timesteps   15 | Total Reward -114\n",
      "Episode    23/  100 | Loss 98593.8369 | Exploration 0.59 | Fruits  1 | Timesteps   36 | Total Reward  266\n",
      "Episode    24/  100 | Loss 28800.5232 | Exploration 0.57 | Fruits  0 | Timesteps    9 | Total Reward -108\n",
      "Episode    25/  100 | Loss 46317.6772 | Exploration 0.55 | Fruits  0 | Timesteps   16 | Total Reward -115\n",
      "Episode    26/  100 | Loss 12321.1023 | Exploration 0.53 | Fruits  0 | Timesteps    5 | Total Reward -104\n",
      "Episode    27/  100 | Loss 14320.8208 | Exploration 0.51 | Fruits  0 | Timesteps    6 | Total Reward -105\n",
      "Episode    28/  100 | Loss 36924.6940 | Exploration 0.50 | Fruits  0 | Timesteps   15 | Total Reward -114\n",
      "Episode    29/  100 | Loss 26089.1305 | Exploration 0.48 | Fruits  0 | Timesteps   10 | Total Reward -109\n",
      "Episode    30/  100 | Loss 38142.7615 | Exploration 0.46 | Fruits  1 | Timesteps   15 | Total Reward  287\n",
      "Episode    31/  100 | Loss 31150.8409 | Exploration 0.44 | Fruits  0 | Timesteps   12 | Total Reward -111\n",
      "Episode    32/  100 | Loss 14017.9629 | Exploration 0.42 | Fruits  0 | Timesteps    7 | Total Reward -106\n",
      "Episode    33/  100 | Loss 41554.7610 | Exploration 0.41 | Fruits  0 | Timesteps   14 | Total Reward -113\n",
      "Episode    34/  100 | Loss 49771.2653 | Exploration 0.39 | Fruits  0 | Timesteps   18 | Total Reward -117\n",
      "Episode    35/  100 | Loss 51516.0449 | Exploration 0.37 | Fruits  0 | Timesteps   18 | Total Reward -117\n",
      "Episode    36/  100 | Loss 19286.5237 | Exploration 0.35 | Fruits  0 | Timesteps    6 | Total Reward -105\n",
      "Episode    37/  100 | Loss 141936.7950 | Exploration 0.33 | Fruits  1 | Timesteps   43 | Total Reward  259\n",
      "Episode    38/  100 | Loss 71172.6719 | Exploration 0.32 | Fruits  0 | Timesteps   27 | Total Reward -126\n",
      "Episode    39/  100 | Loss 45015.6101 | Exploration 0.30 | Fruits  0 | Timesteps   18 | Total Reward -117\n",
      "Episode    40/  100 | Loss 29328.6320 | Exploration 0.28 | Fruits  0 | Timesteps   11 | Total Reward -110\n",
      "Episode    41/  100 | Loss 32972.4678 | Exploration 0.26 | Fruits  0 | Timesteps   11 | Total Reward -110\n",
      "Episode    42/  100 | Loss 37382.3719 | Exploration 0.24 | Fruits  0 | Timesteps   17 | Total Reward -116\n",
      "Episode    43/  100 | Loss 41146.9575 | Exploration 0.23 | Fruits  0 | Timesteps   22 | Total Reward -121\n",
      "Episode    44/  100 | Loss 59674.0154 | Exploration 0.21 | Fruits  0 | Timesteps   29 | Total Reward -128\n",
      "Episode    45/  100 | Loss 40353.3683 | Exploration 0.19 | Fruits  0 | Timesteps   28 | Total Reward -127\n",
      "Episode    46/  100 | Loss 76067.7217 | Exploration 0.17 | Fruits  1 | Timesteps   59 | Total Reward  243\n",
      "Episode    47/  100 | Loss 28057.7844 | Exploration 0.15 | Fruits  0 | Timesteps   19 | Total Reward -118\n",
      "Episode    48/  100 | Loss 10790.4393 | Exploration 0.14 | Fruits  0 | Timesteps    7 | Total Reward -106\n",
      "Episode    49/  100 | Loss 5376.2289 | Exploration 0.12 | Fruits  0 | Timesteps    5 | Total Reward -104\n",
      "Episode    50/  100 | Loss 44045.6171 | Exploration 0.10 | Fruits  0 | Timesteps   38 | Total Reward -137\n",
      "Episode    51/  100 | Loss 79554.7910 | Exploration 0.10 | Fruits  0 | Timesteps   78 | Total Reward -177\n",
      "Episode    52/  100 | Loss 20193.6162 | Exploration 0.10 | Fruits  0 | Timesteps   17 | Total Reward -116\n",
      "Episode    53/  100 | Loss 23893.3852 | Exploration 0.10 | Fruits  0 | Timesteps   27 | Total Reward -126\n",
      "Episode    54/  100 | Loss 25765.9239 | Exploration 0.10 | Fruits  0 | Timesteps   26 | Total Reward -125\n",
      "Episode    55/  100 | Loss 23643.4785 | Exploration 0.10 | Fruits  1 | Timesteps   29 | Total Reward  273\n",
      "Episode    56/  100 | Loss 25951.9600 | Exploration 0.10 | Fruits  0 | Timesteps   29 | Total Reward -128\n",
      "Episode    57/  100 | Loss 28126.3921 | Exploration 0.10 | Fruits  1 | Timesteps   27 | Total Reward  275\n",
      "Episode    58/  100 | Loss 16980.5691 | Exploration 0.10 | Fruits  0 | Timesteps   22 | Total Reward -121\n",
      "Episode    59/  100 | Loss 18014.7000 | Exploration 0.10 | Fruits  0 | Timesteps   20 | Total Reward -119\n",
      "Episode    60/  100 | Loss 48450.0196 | Exploration 0.10 | Fruits  0 | Timesteps   55 | Total Reward -154\n",
      "Episode    61/  100 | Loss 20096.5817 | Exploration 0.10 | Fruits  0 | Timesteps   29 | Total Reward -128\n",
      "Episode    62/  100 | Loss 14760.9485 | Exploration 0.10 | Fruits  1 | Timesteps   20 | Total Reward  282\n",
      "Episode    63/  100 | Loss 17362.5382 | Exploration 0.10 | Fruits  0 | Timesteps   21 | Total Reward -120\n",
      "Episode    64/  100 | Loss 29699.0798 | Exploration 0.10 | Fruits  0 | Timesteps   39 | Total Reward -138\n",
      "Episode    65/  100 | Loss 45012.4550 | Exploration 0.10 | Fruits  0 | Timesteps   56 | Total Reward -155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    66/  100 | Loss 53157.7883 | Exploration 0.10 | Fruits  0 | Timesteps   59 | Total Reward -158\n",
      "Episode    67/  100 | Loss 53062.2732 | Exploration 0.10 | Fruits  1 | Timesteps   62 | Total Reward  240\n",
      "Episode    68/  100 | Loss 8103.4687 | Exploration 0.10 | Fruits  0 | Timesteps   10 | Total Reward -109\n",
      "Episode    69/  100 | Loss 45528.2063 | Exploration 0.10 | Fruits  0 | Timesteps   54 | Total Reward -153\n",
      "Episode    70/  100 | Loss 7953.7781 | Exploration 0.10 | Fruits  0 | Timesteps   10 | Total Reward -109\n",
      "Episode    71/  100 | Loss 24624.8339 | Exploration 0.10 | Fruits  0 | Timesteps   31 | Total Reward -130\n",
      "Episode    72/  100 | Loss 22402.2238 | Exploration 0.10 | Fruits  0 | Timesteps   37 | Total Reward -136\n",
      "Episode    73/  100 | Loss 32283.8262 | Exploration 0.10 | Fruits  1 | Timesteps   39 | Total Reward  263\n",
      "Episode    74/  100 | Loss 46115.5592 | Exploration 0.10 | Fruits  0 | Timesteps   56 | Total Reward -155\n",
      "Episode    75/  100 | Loss 16767.6138 | Exploration 0.10 | Fruits  0 | Timesteps   22 | Total Reward -121\n",
      "Episode    76/  100 | Loss 21685.1405 | Exploration 0.10 | Fruits  0 | Timesteps   30 | Total Reward -129\n",
      "Episode    77/  100 | Loss 22632.4885 | Exploration 0.10 | Fruits  0 | Timesteps   31 | Total Reward -130\n",
      "Episode    78/  100 | Loss 7293.5157 | Exploration 0.10 | Fruits  0 | Timesteps   13 | Total Reward -112\n",
      "Episode    79/  100 | Loss 19331.5540 | Exploration 0.10 | Fruits  0 | Timesteps   24 | Total Reward -123\n",
      "Episode    80/  100 | Loss 16472.5096 | Exploration 0.10 | Fruits  0 | Timesteps   20 | Total Reward -119\n",
      "Episode    81/  100 | Loss 20871.2764 | Exploration 0.10 | Fruits  0 | Timesteps   22 | Total Reward -121\n",
      "Episode    82/  100 | Loss 60248.8222 | Exploration 0.10 | Fruits  0 | Timesteps   78 | Total Reward -177\n",
      "Episode    83/  100 | Loss 10162.4198 | Exploration 0.10 | Fruits  0 | Timesteps   12 | Total Reward -111\n",
      "Episode    84/  100 | Loss 27426.4025 | Exploration 0.10 | Fruits  0 | Timesteps   39 | Total Reward -138\n",
      "Episode    85/  100 | Loss 15629.6415 | Exploration 0.10 | Fruits  0 | Timesteps   27 | Total Reward -126\n",
      "Episode    86/  100 | Loss 18651.2955 | Exploration 0.10 | Fruits  0 | Timesteps   34 | Total Reward -133\n",
      "Episode    87/  100 | Loss 21425.6713 | Exploration 0.10 | Fruits  2 | Timesteps   31 | Total Reward  772\n",
      "Episode    88/  100 | Loss 37403.8249 | Exploration 0.10 | Fruits  0 | Timesteps   47 | Total Reward -146\n",
      "Episode    89/  100 | Loss 14887.3826 | Exploration 0.10 | Fruits  1 | Timesteps   22 | Total Reward  280\n",
      "Episode    90/  100 | Loss 21532.1622 | Exploration 0.10 | Fruits  0 | Timesteps   32 | Total Reward -131\n",
      "Episode    91/  100 | Loss 31593.0515 | Exploration 0.10 | Fruits  0 | Timesteps   39 | Total Reward -138\n",
      "Episode    92/  100 | Loss 40164.9926 | Exploration 0.10 | Fruits  0 | Timesteps   63 | Total Reward -162\n",
      "Episode    93/  100 | Loss 7558.6679 | Exploration 0.10 | Fruits  0 | Timesteps    8 | Total Reward -107\n",
      "Episode    94/  100 | Loss 11291.7625 | Exploration 0.10 | Fruits  0 | Timesteps   13 | Total Reward -112\n",
      "Episode    95/  100 | Loss 100093.4713 | Exploration 0.10 | Fruits  1 | Timesteps  139 | Total Reward  163\n",
      "Episode    96/  100 | Loss 14152.0102 | Exploration 0.10 | Fruits  0 | Timesteps   21 | Total Reward -120\n",
      "Episode    97/  100 | Loss 38420.6387 | Exploration 0.10 | Fruits  0 | Timesteps   53 | Total Reward -152\n",
      "Episode    98/  100 | Loss 8581.1191 | Exploration 0.10 | Fruits  1 | Timesteps   12 | Total Reward  290\n",
      "Episode    99/  100 | Loss 6205.5003 | Exploration 0.10 | Fruits  0 | Timesteps    8 | Total Reward -107\n",
      "Episode   100/  100 | Loss 3328.0310 | Exploration 0.10 | Fruits  0 | Timesteps    7 | Total Reward -106\n"
     ]
    }
   ],
   "source": [
    "#Creating some folders if they don't exist\n",
    "from pathlib import Path\n",
    "Path(\"saved_model\").mkdir(exist_ok=True)\n",
    "Path(\"debug_log\").mkdir(exist_ok=True)\n",
    "Path(\"debug_stats\").mkdir(exist_ok=True)\n",
    "\n",
    "num_episodes = 10000\n",
    "env = create_snake_environment(r\"snakeai/levels/10x10-blank.json\")\n",
    "model = create_dqn_model(env, num_last_frames=1)\n",
    "\n",
    "agent = DeepQNetworkAgent(\n",
    "    model=model,\n",
    "    memory_size=50000,\n",
    "    num_last_frames=model.input_shape[1]\n",
    ")\n",
    "\n",
    "agent.train(\n",
    "    env,\n",
    "    batch_size=128,\n",
    "    num_episodes=num_episodes,\n",
    "    checkpoint_freq=num_episodes // 10,\n",
    "    discount_factor=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End\n"
     ]
    }
   ],
   "source": [
    "from snakeai.gui import PyGameGUI\n",
    "from snakeai.agent import HumanAgent\n",
    "\n",
    "def play_gui(env, agent, num_episodes):\n",
    "    \"\"\"\n",
    "    Play a set of episodes using the specified Snake agent.\n",
    "    Use the interactive graphical interface.\n",
    "    \n",
    "    Args:\n",
    "        env: an instance of Snake environment.\n",
    "        agent: an instance of Snake agent.\n",
    "        num_episodes (int): the number of episodes to run.\n",
    "    \"\"\"\n",
    "\n",
    "    gui = PyGameGUI()\n",
    "    gui.load_environment(env)\n",
    "    gui.load_agent(agent)\n",
    "    gui.run(num_episodes=num_episodes)\n",
    "    print(\"End\")\n",
    "    \n",
    "    \n",
    "env = create_snake_environment(r\"snakeai/levels/10x10-blank.json\")\n",
    "load_model = tf.keras.models.load_model(r\"saved_model/dqn-final.h5\")\n",
    "\n",
    "agent_DQN = DeepQNetworkAgent(model = load_model, memory_size=-1, num_last_frames=1)\n",
    "agent_human = HumanAgent()\n",
    "\n",
    "play_gui(env, agent_DQN, 2) #(environment, agent, number of episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
