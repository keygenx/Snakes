{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class copied from snakeai\\gameplay\\environment.py\n",
    "\n",
    "from snakeai.gameplay.entities import Snake, Field, CellType, SnakeAction, ALL_SNAKE_ACTIONS\n",
    "import pprint\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Environment(object):\n",
    "    \"\"\"\n",
    "    Represents the RL environment for the Snake game that implements the game logic,\n",
    "    provides rewards for the agent and keeps track of game statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, verbose=1):\n",
    "        \"\"\"\n",
    "        Create a new Snake RL environment.\n",
    "        \n",
    "        Args:\n",
    "            config (dict): level configuration, typically found in JSON configs.  \n",
    "            verbose (int): verbosity level:\n",
    "                0 = do not write any debug information;\n",
    "                1 = write a CSV file containing the statistics for every episode;\n",
    "                2 = same as 1, but also write a full log file containing the state of each timestep.\n",
    "        \"\"\"\n",
    "        self.field = Field(level_map=config['field'])\n",
    "        self.snake = None\n",
    "        self.fruit = None\n",
    "        self.initial_snake_length = config['initial_snake_length']\n",
    "        self.rewards = config['rewards']\n",
    "        self.max_step_limit = config.get('max_step_limit', 1000)\n",
    "        self.is_game_over = False\n",
    "\n",
    "        self.timestep_index = 0\n",
    "        self.current_action = None\n",
    "        self.stats = EpisodeStatistics()\n",
    "        self.verbose = verbose\n",
    "        self.debug_file = None\n",
    "        self.stats_file = None\n",
    "\n",
    "    def seed(self, value):\n",
    "        \"\"\" Initialize the random state of the environment to make results reproducible. \"\"\"\n",
    "        random.seed(value)\n",
    "        np.random.seed(value)\n",
    "\n",
    "    @property\n",
    "    def observation_shape(self):\n",
    "        \"\"\" Get the shape of the state observed at each timestep. \"\"\"\n",
    "        return self.field.size_x, self.field.size_y\n",
    "\n",
    "    @property\n",
    "    def num_actions(self):\n",
    "        \"\"\" Get the number of actions the agent can take. \"\"\"\n",
    "        return len(ALL_SNAKE_ACTIONS)\n",
    "\n",
    "    def new_episode(self):\n",
    "        \"\"\" Reset the environment and begin a new episode. \"\"\"\n",
    "        self.field.create_level()\n",
    "        self.stats.reset()\n",
    "        self.timestep_index = 0\n",
    "\n",
    "        self.snake = Snake(self.field.find_snake_head(), length=self.initial_snake_length)\n",
    "        self.field.place_snake(self.snake)\n",
    "        self.generate_fruit()\n",
    "        self.current_action = None\n",
    "        self.is_game_over = False\n",
    "\n",
    "        result = TimestepResult(\n",
    "            observation=self.get_observation(),\n",
    "            reward=0,\n",
    "            is_episode_end=self.is_game_over\n",
    "        )\n",
    "\n",
    "        self.record_timestep_stats(result)\n",
    "        return result\n",
    "\n",
    "    def record_timestep_stats(self, result):\n",
    "        \"\"\" Record environment statistics according to the verbosity level. \"\"\"\n",
    "        timestamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "        # Write CSV header for the stats file.\n",
    "        if self.verbose >= 1 and self.stats_file is None:\n",
    "            self.stats_file = open(f'debug_stats/snake-env-{timestamp}.csv', 'w')\n",
    "            stats_csv_header_line = self.stats.to_dataframe()[:0].to_csv(index=None)\n",
    "            print(stats_csv_header_line, file=self.stats_file, end='', flush=True)\n",
    "\n",
    "        # Create a blank debug log file.\n",
    "        if self.verbose >= 2 and self.debug_file is None:\n",
    "            self.debug_file = open(f'debug_log/snake-env-{timestamp}.log', 'w')\n",
    "\n",
    "        self.stats.record_timestep(self.current_action, result)\n",
    "        self.stats.timesteps_survived = self.timestep_index\n",
    "\n",
    "        if self.verbose >= 2:\n",
    "            print(result, file=self.debug_file)\n",
    "\n",
    "        # Log episode stats if the appropriate verbosity level is set.\n",
    "        if result.is_episode_end:\n",
    "            if self.verbose >= 1:\n",
    "                stats_csv_line = self.stats.to_dataframe().to_csv(header=False, index=None)\n",
    "                print(stats_csv_line, file=self.stats_file, end='', flush=True)\n",
    "            if self.verbose >= 2:\n",
    "                print(self.stats, file=self.debug_file)\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\" Observe the state of the environment. \"\"\"\n",
    "        return np.copy(self.field._cells)\n",
    "\n",
    "    def choose_action(self, action):\n",
    "        \"\"\" Choose the action that will be taken at the next timestep. \"\"\"\n",
    "\n",
    "        self.current_action = action\n",
    "        if action == SnakeAction.TURN_LEFT:\n",
    "            self.snake.turn_left()\n",
    "        elif action == SnakeAction.TURN_RIGHT:\n",
    "            self.snake.turn_right()\n",
    "\n",
    "    def timestep(self):\n",
    "        \"\"\" Execute the timestep and return the new observable state. \"\"\"\n",
    "\n",
    "        self.timestep_index += 1\n",
    "        reward = 0\n",
    "\n",
    "        old_head = self.snake.head\n",
    "        old_tail = self.snake.tail\n",
    "\n",
    "        # Are we about to eat the fruit?\n",
    "        if self.snake.peek_next_move() == self.fruit:\n",
    "            self.snake.grow()\n",
    "            self.generate_fruit()\n",
    "            old_tail = None\n",
    "            reward += self.rewards['ate_fruit'] * self.snake.length\n",
    "            self.stats.fruits_eaten += 1\n",
    "\n",
    "        # If not, just move forward.\n",
    "        else:\n",
    "            self.snake.move()\n",
    "            reward += self.rewards['timestep']\n",
    "\n",
    "        self.field.update_snake_footprint(old_head, old_tail, self.snake.head)\n",
    "\n",
    "        # Hit a wall or own body?\n",
    "        if not self.is_alive():\n",
    "            if self.has_hit_wall():\n",
    "                self.stats.termination_reason = 'hit_wall'\n",
    "            if self.has_hit_own_body():\n",
    "                self.stats.termination_reason = 'hit_own_body'\n",
    "\n",
    "            self.field[self.snake.head] = CellType.SNAKE_HEAD\n",
    "            self.is_game_over = True\n",
    "            reward = self.rewards['died']\n",
    "\n",
    "        # Exceeded the limit of moves?\n",
    "        if self.timestep_index >= self.max_step_limit:\n",
    "            self.is_game_over = True\n",
    "            self.stats.termination_reason = 'timestep_limit_exceeded'\n",
    "\n",
    "        result = TimestepResult(\n",
    "            observation=self.get_observation(),\n",
    "            reward=reward,\n",
    "            is_episode_end=self.is_game_over\n",
    "        )\n",
    "\n",
    "        self.record_timestep_stats(result)\n",
    "        return result\n",
    "\n",
    "    def generate_fruit(self, position=None):\n",
    "        \"\"\" Generate a new fruit at a random unoccupied cell. \"\"\"\n",
    "        if position is None:\n",
    "            position = self.field.get_random_empty_cell()\n",
    "        self.field[position] = CellType.FRUIT\n",
    "        self.fruit = position\n",
    "\n",
    "    def has_hit_wall(self):\n",
    "        \"\"\" True if the snake has hit a wall, False otherwise. \"\"\"\n",
    "        return self.field[self.snake.head] == CellType.WALL\n",
    "\n",
    "    def has_hit_own_body(self):\n",
    "        \"\"\" True if the snake has hit its own body, False otherwise. \"\"\"\n",
    "        return self.field[self.snake.head] == CellType.SNAKE_BODY\n",
    "\n",
    "    def is_alive(self):\n",
    "        \"\"\" True if the snake is still alive, False otherwise. \"\"\"\n",
    "        return not self.has_hit_wall() and not self.has_hit_own_body()\n",
    "\n",
    "\n",
    "class TimestepResult(object):\n",
    "    \"\"\" Represents the information provided to the agent after each timestep. \"\"\"\n",
    "\n",
    "    def __init__(self, observation, reward, is_episode_end):\n",
    "        self.observation = observation\n",
    "        self.reward = reward\n",
    "        self.is_episode_end = is_episode_end\n",
    "\n",
    "    def __str__(self):\n",
    "        field_map = '\\n'.join([\n",
    "            ''.join(str(cell) for cell in row)\n",
    "            for row in self.observation\n",
    "        ])\n",
    "        return f'{field_map}\\nR = {self.reward}   end={self.is_episode_end}\\n'\n",
    "\n",
    "\n",
    "class EpisodeStatistics(object):\n",
    "    \"\"\" Represents the summary of the agent's performance during the episode. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Forget all previous statistics and prepare for a new episode. \"\"\"\n",
    "        self.timesteps_survived = 0\n",
    "        self.sum_episode_rewards = 0\n",
    "        self.fruits_eaten = 0\n",
    "        self.termination_reason = None\n",
    "        self.action_counter = {\n",
    "            action: 0\n",
    "            for action in ALL_SNAKE_ACTIONS\n",
    "        }\n",
    "\n",
    "    def record_timestep(self, action, result):\n",
    "        \"\"\" Update the stats based on the current timestep results. \"\"\"\n",
    "        self.sum_episode_rewards += result.reward\n",
    "        if action is not None:\n",
    "            self.action_counter[action] += 1\n",
    "\n",
    "    def flatten(self):\n",
    "        \"\"\" Format all episode statistics as a flat object. \"\"\"\n",
    "        flat_stats = {\n",
    "            'timesteps_survived': self.timesteps_survived,\n",
    "            'sum_episode_rewards': self.sum_episode_rewards,\n",
    "            'mean_reward': self.sum_episode_rewards / self.timesteps_survived if self.timesteps_survived else None,\n",
    "            'fruits_eaten': self.fruits_eaten,\n",
    "            'termination_reason': self.termination_reason,\n",
    "        }\n",
    "        flat_stats.update({\n",
    "            f'action_counter_{action}': self.action_counter.get(action, 0)\n",
    "            for action in ALL_SNAKE_ACTIONS\n",
    "        })\n",
    "        return flat_stats\n",
    "\n",
    "    def to_dataframe(self):\n",
    "        \"\"\" Convert the episode statistics to a Pandas data frame. \"\"\"\n",
    "        return pd.DataFrame([self.flatten()])\n",
    "\n",
    "    def __str__(self):\n",
    "        return pprint.pformat(self.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions and Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snake_environment(level_filename):\n",
    "    \"\"\" Create a new Snake environment from the config file. \"\"\"\n",
    "\n",
    "    with open(level_filename) as cfg:\n",
    "        env_config = json.load(cfg)\n",
    "\n",
    "    return Environment(config=env_config, verbose=1)\n",
    "\n",
    "\n",
    "def create_dqn_model(env, num_last_frames):\n",
    "    \"\"\"\n",
    "    Build a new DQN model to be used for training.\n",
    "    \n",
    "    Args:\n",
    "        env: an instance of Snake environment. \n",
    "        num_last_frames: the number of last frames the agent considers as state.\n",
    "\n",
    "    Returns:\n",
    "        A compiled DQN model.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutions.\n",
    "    model.add(Conv2D(\n",
    "        16,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        data_format='channels_first',\n",
    "        input_shape=(num_last_frames, ) + env.observation_shape\n",
    "    ))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(\n",
    "        32,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        data_format='channels_first'\n",
    "    ))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Dense layers.\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(env.num_actions))\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(RMSprop(), 'MSE')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExperienceReplay Class from snakeai\\utils\\memory.py\n",
    "import collections #To use Deque datatype for buffer.\n",
    "import random #for random sampling from the buffer. Buffer is a Deque datatype\n",
    "\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    \"\"\" Represents the experience replay memory that can be randomly sampled. \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape, num_actions, memory_size=100):\n",
    "        \"\"\"\n",
    "        Create a new instance of experience replay memory.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: the shape of the agent state.\n",
    "            num_actions: the number of actions allowed in the environment.\n",
    "            memory_size: memory size limit (-1 for unlimited).\n",
    "        \"\"\"\n",
    "        self.memory = collections.deque()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.memory_size = memory_size\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Erase the experience replay memory. \"\"\"\n",
    "        self.memory = collections.deque()\n",
    "\n",
    "    def remember(self, state, action, reward, state_next, is_episode_end):\n",
    "        \"\"\"\n",
    "        Store a new piece of experience into the replay memory.\n",
    "        \n",
    "        Args:\n",
    "            state: state observed at the previous step.\n",
    "            action: action taken at the previous step.\n",
    "            reward: reward received at the beginning of the current step.\n",
    "            state_next: state observed at the current step. \n",
    "            is_episode_end: whether the episode has ended with the current step.\n",
    "        \"\"\"\n",
    "        memory_item = np.concatenate([\n",
    "            state.flatten(),\n",
    "            np.array(action).flatten(),\n",
    "            np.array(reward).flatten(),\n",
    "            state_next.flatten(),\n",
    "            1 * np.array(is_episode_end).flatten()\n",
    "        ])\n",
    "        self.memory.append(memory_item)\n",
    "        if 0 < self.memory_size < len(self.memory):\n",
    "            self.memory.popleft()\n",
    "\n",
    "    def get_batch(self, model, batch_size, discount_factor=0.9):\n",
    "        \"\"\" Sample a batch from experience replay. \"\"\"\n",
    "\n",
    "        batch_size = min(len(self.memory), batch_size)\n",
    "        experience = np.array(random.sample(self.memory, batch_size))\n",
    "        input_dim = np.prod(self.input_shape)\n",
    "\n",
    "        # Extract [S, a, r, S', end] from experience.\n",
    "        states = experience[:, 0:input_dim]\n",
    "        actions = experience[:, input_dim]\n",
    "        rewards = experience[:, input_dim + 1]\n",
    "        states_next = experience[:, input_dim + 2:2 * input_dim + 2]\n",
    "        episode_ends = experience[:, 2 * input_dim + 2]\n",
    "\n",
    "        # Reshape to match the batch structure.\n",
    "        states = states.reshape((batch_size, ) + self.input_shape)\n",
    "        actions = np.cast['int'](actions)\n",
    "        rewards = rewards.repeat(self.num_actions).reshape((batch_size, self.num_actions))\n",
    "        states_next = states_next.reshape((batch_size, ) + self.input_shape)\n",
    "        episode_ends = episode_ends.repeat(self.num_actions).reshape((batch_size, self.num_actions))\n",
    "\n",
    "        # Predict future state-action values.\n",
    "        X = np.concatenate([states, states_next], axis=0)\n",
    "        y = model.predict(X)\n",
    "        Q_next = np.max(y[batch_size:], axis=1).repeat(self.num_actions).reshape((batch_size, self.num_actions))\n",
    "\n",
    "        delta = np.zeros((batch_size, self.num_actions))\n",
    "        delta[np.arange(batch_size), actions] = 1\n",
    "\n",
    "        targets = (1 - delta) * y[:batch_size] + delta * (rewards + discount_factor * (1 - episode_ends) * Q_next)\n",
    "        return states, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DeepQNetworkAgent Class from snakeai\\agent\\dqn.py\n",
    "class DeepQNetworkAgent():\n",
    "    \"\"\" Represents a Snake agent powered by DQN with experience replay. \"\"\"\n",
    "\n",
    "    def __init__(self, model, num_last_frames=4, memory_size=1000):\n",
    "        \"\"\"\n",
    "        Create a new DQN-based agent.\n",
    "        \n",
    "        Args:\n",
    "            model: a compiled DQN model.\n",
    "            num_last_frames (int): the number of last frames the agent will consider.\n",
    "            memory_size (int): memory size limit for experience replay (-1 for unlimited). \n",
    "        \"\"\"\n",
    "        assert model.input_shape[1] == num_last_frames, 'Model input shape should be (num_frames, grid_size, grid_size)'\n",
    "        assert len(model.output_shape) == 2, 'Model output shape should be (num_samples, num_actions)'\n",
    "\n",
    "        self.model = model\n",
    "        self.num_last_frames = num_last_frames\n",
    "        self.memory = ExperienceReplay((num_last_frames,) + model.input_shape[-2:], model.output_shape[-1], memory_size)\n",
    "        self.frames = None\n",
    "\n",
    "    def begin_episode(self):\n",
    "        \"\"\" Reset the agent for a new episode. \"\"\"\n",
    "        self.frames = None\n",
    "\n",
    "    def get_last_frames(self, observation):\n",
    "        \"\"\"\n",
    "        Get the pixels of the last `num_last_frames` observations, the current frame being the last.\n",
    "        \n",
    "        Args:\n",
    "            observation: observation at the current timestep. \n",
    "\n",
    "        Returns:\n",
    "            Observations for the last `num_last_frames` frames.\n",
    "        \"\"\"\n",
    "        frame = observation\n",
    "        if self.frames is None:\n",
    "            self.frames = collections.deque([frame] * self.num_last_frames)\n",
    "        else:\n",
    "            self.frames.append(frame)\n",
    "            self.frames.popleft()\n",
    "        return np.expand_dims(self.frames, 0)\n",
    "\n",
    "    def train(self, env, num_episodes=1000, batch_size=50, discount_factor=0.9, checkpoint_freq=None,\n",
    "              exploration_range=(1.0, 0.1), exploration_phase_size=0.5):\n",
    "        \"\"\"\n",
    "        Train the agent to perform well in the given Snake environment.\n",
    "        \n",
    "        Args:\n",
    "            env:\n",
    "                an instance of Snake environment.\n",
    "            num_episodes (int):\n",
    "                the number of episodes to run during the training.\n",
    "            batch_size (int):\n",
    "                the size of the learning sample for experience replay.\n",
    "            discount_factor (float):\n",
    "                discount factor (gamma) for computing the value function.\n",
    "            checkpoint_freq (int):\n",
    "                the number of episodes after which a new model checkpoint will be created.\n",
    "            exploration_range (tuple):\n",
    "                a (max, min) range specifying how the exploration rate should decay over time. \n",
    "            exploration_phase_size (float):\n",
    "                the percentage of the training process at which\n",
    "                the exploration rate should reach its minimum.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the constant exploration decay speed for each episode.\n",
    "        max_exploration_rate, min_exploration_rate = exploration_range\n",
    "        exploration_decay = ((max_exploration_rate - min_exploration_rate) / (num_episodes * exploration_phase_size))\n",
    "        exploration_rate = max_exploration_rate\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            # Reset the environment for the new episode.\n",
    "            timestep = env.new_episode()\n",
    "            self.begin_episode()\n",
    "            game_over = False\n",
    "            loss = 0.0\n",
    "\n",
    "            # Observe the initial state.\n",
    "            state = self.get_last_frames(timestep.observation)\n",
    "\n",
    "            while not game_over:\n",
    "                if np.random.random() < exploration_rate:\n",
    "                    # Explore: take a random action.\n",
    "                    action = np.random.randint(env.num_actions)\n",
    "                else:\n",
    "                    # Exploit: take the best known action for this state.\n",
    "                    q = self.model.predict(state)\n",
    "                    action = np.argmax(q[0])\n",
    "\n",
    "                # Act on the environment.\n",
    "                env.choose_action(action)\n",
    "                timestep = env.timestep()\n",
    "\n",
    "                # Remember a new piece of experience.\n",
    "                reward = timestep.reward\n",
    "                state_next = self.get_last_frames(timestep.observation)\n",
    "                game_over = timestep.is_episode_end\n",
    "                experience_item = [state, action, reward, state_next, game_over]\n",
    "                self.memory.remember(*experience_item)\n",
    "                state = state_next\n",
    "\n",
    "                # Sample a random batch from experience.\n",
    "                batch = self.memory.get_batch(\n",
    "                    model=self.model,\n",
    "                    batch_size=batch_size,\n",
    "                    discount_factor=discount_factor\n",
    "                )\n",
    "\n",
    "                # Learn on the batch.\n",
    "                if batch:\n",
    "                    inputs, targets = batch\n",
    "                    loss += float(self.model.train_on_batch(inputs, targets))\n",
    "\n",
    "            if checkpoint_freq and (episode % checkpoint_freq) == 0:\n",
    "                self.model.save(f'saved_model/dqn-{episode:08d}.h5')\n",
    "\n",
    "            if exploration_rate > min_exploration_rate:\n",
    "                exploration_rate -= exploration_decay\n",
    "\n",
    "            summary = 'Episode {:5d}/{:5d} | Loss {:8.4f} | Exploration {:.2f} | ' + \\\n",
    "                      'Fruits {:2d} | Timesteps {:4d} | Total Reward {:4d}'\n",
    "            print(summary.format(\n",
    "                episode + 1, num_episodes, loss, exploration_rate,\n",
    "                env.stats.fruits_eaten, env.stats.timesteps_survived, env.stats.sum_episode_rewards,\n",
    "            ))\n",
    "\n",
    "        self.model.save('saved_model/dqn-final.h5')\n",
    "\n",
    "    def act(self, observation, reward):\n",
    "        \"\"\"\n",
    "        Choose the next action to take.\n",
    "        \n",
    "        Args:\n",
    "            observation: observable state for the current timestep. \n",
    "            reward: reward received at the beginning of the current timestep.\n",
    "\n",
    "        Returns:\n",
    "            The index of the action to take next.\n",
    "        \"\"\"\n",
    "        state = self.get_last_frames(observation)\n",
    "        q = self.model.predict(state)[0]\n",
    "        return np.argmax(q)\n",
    "    \n",
    "    def end_episode(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 16, 8, 8)          592       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 6, 6)          4640      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 32, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               295168    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 301,171\n",
      "Trainable params: 301,171\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode     1/   10 | Loss   7.5721 | Exploration 0.82 | Fruits  0 | Timesteps   10 | Total Reward   -1\n",
      "Episode     2/   10 | Loss   1.2870 | Exploration 0.64 | Fruits  1 | Timesteps   10 | Total Reward    3\n",
      "Episode     3/   10 | Loss  23.6057 | Exploration 0.46 | Fruits  0 | Timesteps   10 | Total Reward   -1\n",
      "Episode     4/   10 | Loss   9.8662 | Exploration 0.28 | Fruits  0 | Timesteps   23 | Total Reward   -1\n",
      "Episode     5/   10 | Loss   2.4016 | Exploration 0.10 | Fruits  0 | Timesteps   20 | Total Reward   -1\n",
      "Episode     6/   10 | Loss   1.5704 | Exploration -0.08 | Fruits  0 | Timesteps   13 | Total Reward   -1\n",
      "Episode     7/   10 | Loss   2.4408 | Exploration -0.08 | Fruits  0 | Timesteps   53 | Total Reward   -1\n",
      "Episode     8/   10 | Loss   5.5820 | Exploration -0.08 | Fruits  0 | Timesteps 1000 | Total Reward    0\n",
      "Episode     9/   10 | Loss   0.0204 | Exploration -0.08 | Fruits  0 | Timesteps   11 | Total Reward   -1\n",
      "Episode    10/   10 | Loss   0.2691 | Exploration -0.08 | Fruits  2 | Timesteps   36 | Total Reward    8\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "Path(\"saved_model\").mkdir(exist_ok=True)\n",
    "\n",
    "num_episodes = 10\n",
    "env = create_snake_environment(r\"snakeai/levels/10x10-blank.json\")\n",
    "model = create_dqn_model(env, num_last_frames=4)\n",
    "\n",
    "agent = DeepQNetworkAgent(\n",
    "    model=model,\n",
    "    memory_size=-1,\n",
    "    num_last_frames=model.input_shape[1]\n",
    ")\n",
    "\n",
    "agent.train(\n",
    "    env,\n",
    "    batch_size=64,\n",
    "    num_episodes=num_episodes,\n",
    "    checkpoint_freq=num_episodes // 10,\n",
    "    discount_factor=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "from snakeai.gui import PyGameGUI\n",
    "from snakeai.agent import HumanAgent\n",
    "\n",
    "def play_gui(env, agent, num_episodes):\n",
    "    \"\"\"\n",
    "    Play a set of episodes using the specified Snake agent.\n",
    "    Use the interactive graphical interface.\n",
    "    \n",
    "    Args:\n",
    "        env: an instance of Snake environment.\n",
    "        agent: an instance of Snake agent.\n",
    "        num_episodes (int): the number of episodes to run.\n",
    "    \"\"\"\n",
    "\n",
    "    gui = PyGameGUI()\n",
    "    gui.load_environment(env)\n",
    "    gui.load_agent(agent)\n",
    "    gui.run(num_episodes=num_episodes)\n",
    "    print(\"End\")\n",
    "    \n",
    "    \n",
    "env = create_snake_environment(r\"snakeai/levels/10x10-blank.json\")\n",
    "load_model = tf.keras.models.load_model(r\"saved_model/dqn-final.h5\")\n",
    "\n",
    "agent = DeepQNetworkAgent(model=load_model, memory_size=-1, num_last_frames=4)\n",
    "\n",
    "#play_gui(env, agent, 2) #(environment, agent, number of episodes)\n",
    "play_gui(env, HumanAgent(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
